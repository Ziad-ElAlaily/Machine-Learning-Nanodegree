{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Make and Model Recognizer\n",
    "I shall explore here my tries to reach a good optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import random\n",
    "from glob import glob\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from random import sample\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8144 images belonging to 196 classes.\n",
      "Found 8041 images belonging to 196 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   zoom_range=0.25,\n",
    "                                   rotation_range = 90,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 horizontal_flip = True)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory('My_Cars/train',\n",
    "                                              target_size=(128,128),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')\n",
    "test_data = test_datagen.flow_from_directory('My_Cars/test',\n",
    "                                              target_size=(128,128),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def modelFunction(filters,kernel_size,strides,pool_size):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     #First Layer\n",
    "#     model.add(Conv2D(filters=filters, kernel_size=kernel_size,padding='same', input_shape=(128, 128, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "#     #model.add(Dropout(0.25))\n",
    "#     #Second Layer\n",
    "#     model.add(Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "#     #model.add(Dropout(0.25))\n",
    "#     #Third Layer\n",
    "#     model.add(Conv2D(filters=filters*4, kernel_size=kernel_size, activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     #Fourth Layer\n",
    "#     # model.add(Conv2D(filters=256, kernel_size=2,padding='same', activation='relu'))\n",
    "#     # model.add(MaxPooling2D(pool_size=2,strides = 4))\n",
    "#     #model.add(Dropout(0.25))\n",
    "\n",
    "#     #model.add(GlobalAveragePooling2D())\n",
    "#     #Last Layer\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(196, activation='softmax'))\n",
    "\n",
    "#     model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 poolSize/2h strides \n",
      "WARNING:tensorflow:From C:\\Users\\CostaPC2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\CostaPC2\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\CostaPC2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 5.3903 - acc: 0.0056 - val_loss: 5.2777 - val_acc: 0.0081\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.27770, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 63s 629ms/step - loss: 5.2716 - acc: 0.0069 - val_loss: 5.2177 - val_acc: 0.0109\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.27770 to 5.21771, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 5.2296 - acc: 0.0091 - val_loss: 5.2053 - val_acc: 0.0132\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.21771 to 5.20535, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 5.2105 - acc: 0.0100 - val_loss: 5.1655 - val_acc: 0.0122\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.20535 to 5.16547, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 5.1958 - acc: 0.0119 - val_loss: 5.2623 - val_acc: 0.0103\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.16547\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 5.1789 - acc: 0.0122 - val_loss: 5.1189 - val_acc: 0.0161\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.16547 to 5.11886, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 5.1464 - acc: 0.0138 - val_loss: 5.1192 - val_acc: 0.0125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.11886\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 5.1410 - acc: 0.0159 - val_loss: 5.1022 - val_acc: 0.0186\n",
      "\n",
      "Epoch 00008: val_loss improved from 5.11886 to 5.10222, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 5.0809 - acc: 0.0166 - val_loss: 5.0915 - val_acc: 0.0178\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.10222 to 5.09149, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 68s 676ms/step - loss: 5.0870 - acc: 0.0181 - val_loss: 5.0666 - val_acc: 0.0181\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.09149 to 5.06657, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 64s 639ms/step - loss: 5.0515 - acc: 0.0188 - val_loss: 5.0983 - val_acc: 0.0239\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.06657\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 62s 625ms/step - loss: 5.0310 - acc: 0.0253 - val_loss: 5.0941 - val_acc: 0.0272\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.06657\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 65s 653ms/step - loss: 5.0255 - acc: 0.0253 - val_loss: 5.0329 - val_acc: 0.0274\n",
      "\n",
      "Epoch 00013: val_loss improved from 5.06657 to 5.03292, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 4.9989 - acc: 0.0272 - val_loss: 5.0743 - val_acc: 0.0256\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.03292\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 59s 595ms/step - loss: 4.9795 - acc: 0.0325 - val_loss: 5.0154 - val_acc: 0.0269\n",
      "\n",
      "Epoch 00015: val_loss improved from 5.03292 to 5.01542, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 4.9270 - acc: 0.0394 - val_loss: 5.0508 - val_acc: 0.0315\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.01542\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 4.9362 - acc: 0.0366 - val_loss: 4.9890 - val_acc: 0.0347\n",
      "\n",
      "Epoch 00017: val_loss improved from 5.01542 to 4.98901, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 4.9432 - acc: 0.0334 - val_loss: 5.1441 - val_acc: 0.0296\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.98901\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 61s 606ms/step - loss: 4.8546 - acc: 0.0441 - val_loss: 4.9754 - val_acc: 0.0387\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.98901 to 4.97540, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 60s 595ms/step - loss: 4.8411 - acc: 0.0384 - val_loss: 4.9925 - val_acc: 0.0328\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.97540\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 4.8736 - acc: 0.0463 - val_loss: 5.2010 - val_acc: 0.0305\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 4.97540\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 4.8126 - acc: 0.0447 - val_loss: 4.8997 - val_acc: 0.0394\n",
      "\n",
      "Epoch 00022: val_loss improved from 4.97540 to 4.89973, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 61s 606ms/step - loss: 4.8180 - acc: 0.0450 - val_loss: 4.9490 - val_acc: 0.0340\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 4.89973\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 4.7526 - acc: 0.0531 - val_loss: 4.9101 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.89973\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 4.7724 - acc: 0.0537 - val_loss: 5.0729 - val_acc: 0.0397\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.89973\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 4.7687 - acc: 0.0531 - val_loss: 5.0681 - val_acc: 0.0312\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.89973\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 4.7528 - acc: 0.0544 - val_loss: 4.9386 - val_acc: 0.0384\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.89973\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 62s 615ms/step - loss: 4.7585 - acc: 0.0606 - val_loss: 4.9251 - val_acc: 0.0444\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 4.89973\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 4.6585 - acc: 0.0688 - val_loss: 4.8810 - val_acc: 0.0425\n",
      "\n",
      "Epoch 00029: val_loss improved from 4.89973 to 4.88097, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 4.7302 - acc: 0.0553 - val_loss: 5.0757 - val_acc: 0.0413\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4.88097\n",
      "saving new best model, val_loss = 4.880966601371765\n",
      "4 poolSize/2h strides \n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 65s 646ms/step - loss: 5.3641 - acc: 0.0047 - val_loss: 5.2464 - val_acc: 0.0085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.24638, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 5.2393 - acc: 0.0084 - val_loss: 5.1725 - val_acc: 0.0150\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.24638 to 5.17255, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 59s 590ms/step - loss: 5.1910 - acc: 0.0144 - val_loss: 5.1494 - val_acc: 0.0179\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.17255 to 5.14945, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 5.1518 - acc: 0.0147 - val_loss: 5.1385 - val_acc: 0.0112\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.14945 to 5.13849, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 5.1403 - acc: 0.0194 - val_loss: 5.1010 - val_acc: 0.0225\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.13849 to 5.10101, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 61s 610ms/step - loss: 5.1041 - acc: 0.0197 - val_loss: 5.1815 - val_acc: 0.0246\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 5.10101\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 5.0679 - acc: 0.0247 - val_loss: 5.0409 - val_acc: 0.0225\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.10101 to 5.04091, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 5.0633 - acc: 0.0247 - val_loss: 5.0615 - val_acc: 0.0283\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.04091\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 4.9905 - acc: 0.0297 - val_loss: 5.0179 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.04091 to 5.01795, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 5.0138 - acc: 0.0275 - val_loss: 4.9823 - val_acc: 0.0294\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.01795 to 4.98232, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 4.9951 - acc: 0.0281 - val_loss: 5.0323 - val_acc: 0.0274\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.98232\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 4.9196 - acc: 0.0331 - val_loss: 5.8812 - val_acc: 0.0216\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.98232\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 4.9479 - acc: 0.0341 - val_loss: 5.0896 - val_acc: 0.0368\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.98232\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 4.9246 - acc: 0.0300 - val_loss: 4.9727 - val_acc: 0.0341\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.98232 to 4.97265, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 4.8695 - acc: 0.0378 - val_loss: 5.2104 - val_acc: 0.0306\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4.97265\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 4.8623 - acc: 0.0388 - val_loss: 4.9613 - val_acc: 0.0456\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.97265 to 4.96134, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 61s 611ms/step - loss: 4.8347 - acc: 0.0488 - val_loss: 4.9735 - val_acc: 0.0397\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4.96134\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 4.8214 - acc: 0.0450 - val_loss: 5.0579 - val_acc: 0.0434\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.96134\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 62s 618ms/step - loss: 4.8184 - acc: 0.0478 - val_loss: 5.2127 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 4.96134\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 4.8354 - acc: 0.0469 - val_loss: 4.9735 - val_acc: 0.0416\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.96134\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 4.8192 - acc: 0.0481 - val_loss: 4.9406 - val_acc: 0.0460\n",
      "\n",
      "Epoch 00021: val_loss improved from 4.96134 to 4.94063, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 4.7699 - acc: 0.0531 - val_loss: 5.1323 - val_acc: 0.0372\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.94063\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 4.7745 - acc: 0.0500 - val_loss: 4.8356 - val_acc: 0.0513\n",
      "\n",
      "Epoch 00023: val_loss improved from 4.94063 to 4.83558, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 4.7373 - acc: 0.0569 - val_loss: 4.9394 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.83558\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 61s 611ms/step - loss: 4.7130 - acc: 0.0587 - val_loss: 5.5555 - val_acc: 0.0409\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.83558\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 4.7771 - acc: 0.0491 - val_loss: 4.9447 - val_acc: 0.0428\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.83558\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 4.6832 - acc: 0.0566 - val_loss: 5.0946 - val_acc: 0.0484\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.83558\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 61s 610ms/step - loss: 4.7003 - acc: 0.0531 - val_loss: 4.8163 - val_acc: 0.0497\n",
      "\n",
      "Epoch 00028: val_loss improved from 4.83558 to 4.81631, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 60s 601ms/step - loss: 4.6950 - acc: 0.0591 - val_loss: 5.3600 - val_acc: 0.0403\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4.81631\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 4.7180 - acc: 0.0528 - val_loss: 4.7954 - val_acc: 0.0541\n",
      "\n",
      "Epoch 00030: val_loss improved from 4.81631 to 4.79544, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "saving new best model, val_loss = 4.795438079833985\n",
      "6 poolSize/2h strides \n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 65s 651ms/step - loss: 5.3651 - acc: 0.0025 - val_loss: 5.2426 - val_acc: 0.0076\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.24263, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 5.2386 - acc: 0.0097 - val_loss: 5.1868 - val_acc: 0.0131\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.24263 to 5.18678, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 5.2168 - acc: 0.0122 - val_loss: 5.3847 - val_acc: 0.0113\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.18678\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 5.1935 - acc: 0.0137 - val_loss: 5.1813 - val_acc: 0.0141\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.18678 to 5.18129, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 5.1728 - acc: 0.0175 - val_loss: 5.1434 - val_acc: 0.0172\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.18129 to 5.14336, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 61s 611ms/step - loss: 5.1652 - acc: 0.0134 - val_loss: 5.1000 - val_acc: 0.0233\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.14336 to 5.09999, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 5.1107 - acc: 0.0184 - val_loss: 5.1201 - val_acc: 0.0231\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.09999\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 5.1179 - acc: 0.0206 - val_loss: 5.0598 - val_acc: 0.0275\n",
      "\n",
      "Epoch 00008: val_loss improved from 5.09999 to 5.05979, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 60s 599ms/step - loss: 5.1000 - acc: 0.0250 - val_loss: 5.1326 - val_acc: 0.0223\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.05979\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 5.1184 - acc: 0.0169 - val_loss: 5.0361 - val_acc: 0.0272\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.05979 to 5.03614, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 61s 606ms/step - loss: 5.0555 - acc: 0.0275 - val_loss: 5.0640 - val_acc: 0.0271\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.03614\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 5.0314 - acc: 0.0244 - val_loss: 5.0628 - val_acc: 0.0284\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.03614\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 5.0415 - acc: 0.0256 - val_loss: 5.0775 - val_acc: 0.0238\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.03614\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 5.0457 - acc: 0.0241 - val_loss: 5.1133 - val_acc: 0.0277\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.03614\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 59s 591ms/step - loss: 5.0168 - acc: 0.0300 - val_loss: 5.0205 - val_acc: 0.0247\n",
      "\n",
      "Epoch 00015: val_loss improved from 5.03614 to 5.02052, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 5.0141 - acc: 0.0284 - val_loss: 5.0861 - val_acc: 0.0290\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.02052\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 4.9673 - acc: 0.0338 - val_loss: 5.1111 - val_acc: 0.0325\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.02052\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 4.9736 - acc: 0.0316 - val_loss: 5.0838 - val_acc: 0.0369\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.02052\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 4.9824 - acc: 0.0303 - val_loss: 5.0206 - val_acc: 0.0343\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.02052\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 4.9768 - acc: 0.0362 - val_loss: 5.0937 - val_acc: 0.0284\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.02052\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 4.9512 - acc: 0.0384 - val_loss: 5.0048 - val_acc: 0.0302\n",
      "\n",
      "Epoch 00021: val_loss improved from 5.02052 to 5.00481, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 58s 576ms/step - loss: 4.9217 - acc: 0.0388 - val_loss: 5.0632 - val_acc: 0.0400\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5.00481\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 4.9093 - acc: 0.0381 - val_loss: 5.0428 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 5.00481\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 55s 554ms/step - loss: 4.9543 - acc: 0.0384 - val_loss: 4.9961 - val_acc: 0.0381\n",
      "\n",
      "Epoch 00024: val_loss improved from 5.00481 to 4.99612, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 4.9054 - acc: 0.0384 - val_loss: 5.1659 - val_acc: 0.0306\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.99612\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 4.9234 - acc: 0.0416 - val_loss: 5.0109 - val_acc: 0.0384\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.99612\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 58s 575ms/step - loss: 4.8889 - acc: 0.0422 - val_loss: 5.0627 - val_acc: 0.0338\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.99612\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 58s 575ms/step - loss: 4.8804 - acc: 0.0441 - val_loss: 5.0454 - val_acc: 0.0356\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 4.99612\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 55s 555ms/step - loss: 4.9262 - acc: 0.0316 - val_loss: 5.1506 - val_acc: 0.0327\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4.99612\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 4.8610 - acc: 0.0478 - val_loss: 5.1417 - val_acc: 0.0328\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4.99612\n",
      "2 poolSize/4h strides \n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 5.2822 - acc: 0.0066 - val_loss: 5.2546 - val_acc: 0.0107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.25460, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 5.2394 - acc: 0.0128 - val_loss: 5.1857 - val_acc: 0.0134\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.25460 to 5.18573, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 57s 568ms/step - loss: 5.2176 - acc: 0.0091 - val_loss: 5.1849 - val_acc: 0.0128\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.18573 to 5.18486, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 5.1841 - acc: 0.0106 - val_loss: 5.1590 - val_acc: 0.0113\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.18486 to 5.15904, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 58s 582ms/step - loss: 5.1551 - acc: 0.0144 - val_loss: 5.1584 - val_acc: 0.0125\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.15904 to 5.15838, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 62s 615ms/step - loss: 5.1541 - acc: 0.0163 - val_loss: 5.1259 - val_acc: 0.0208\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.15838 to 5.12585, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 58s 578ms/step - loss: 5.1238 - acc: 0.0166 - val_loss: 5.0883 - val_acc: 0.0219\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.12585 to 5.08832, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 58s 582ms/step - loss: 5.0989 - acc: 0.0172 - val_loss: 5.0927 - val_acc: 0.0222\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.08832\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 65s 649ms/step - loss: 5.1156 - acc: 0.0187 - val_loss: 5.1238 - val_acc: 0.0094\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.08832\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 5.0655 - acc: 0.0206 - val_loss: 5.0763 - val_acc: 0.0225\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.08832 to 5.07634, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 5.0700 - acc: 0.0247 - val_loss: 5.0430 - val_acc: 0.0236\n",
      "\n",
      "Epoch 00011: val_loss improved from 5.07634 to 5.04305, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 61s 615ms/step - loss: 5.0607 - acc: 0.0256 - val_loss: 5.0302 - val_acc: 0.0222\n",
      "\n",
      "Epoch 00012: val_loss improved from 5.04305 to 5.03023, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 5.0180 - acc: 0.0256 - val_loss: 5.0623 - val_acc: 0.0256\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.03023\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 5.0373 - acc: 0.0281 - val_loss: 5.0002 - val_acc: 0.0268\n",
      "\n",
      "Epoch 00014: val_loss improved from 5.03023 to 5.00018, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 62s 615ms/step - loss: 5.0160 - acc: 0.0263 - val_loss: 5.0290 - val_acc: 0.0241\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.00018\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 5.0072 - acc: 0.0281 - val_loss: 5.0058 - val_acc: 0.0296\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.00018\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 62s 620ms/step - loss: 4.9768 - acc: 0.0359 - val_loss: 5.0181 - val_acc: 0.0306\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.00018\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 4.9761 - acc: 0.0266 - val_loss: 5.0140 - val_acc: 0.0328\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.00018\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 4.9802 - acc: 0.0291 - val_loss: 4.9624 - val_acc: 0.0321\n",
      "\n",
      "Epoch 00019: val_loss improved from 5.00018 to 4.96240, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 56s 559ms/step - loss: 4.9510 - acc: 0.0366 - val_loss: 5.0360 - val_acc: 0.0300\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.96240\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 4.9397 - acc: 0.0338 - val_loss: 4.9894 - val_acc: 0.0321\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 4.96240\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 4.9371 - acc: 0.0344 - val_loss: 5.0068 - val_acc: 0.0303\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.96240\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 4.9190 - acc: 0.0338 - val_loss: 4.9298 - val_acc: 0.0359\n",
      "\n",
      "Epoch 00023: val_loss improved from 4.96240 to 4.92975, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 4.9285 - acc: 0.0384 - val_loss: 4.9591 - val_acc: 0.0337\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.92975\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 4.9197 - acc: 0.0366 - val_loss: 4.9299 - val_acc: 0.0431\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.92975\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 4.8740 - acc: 0.0428 - val_loss: 4.9516 - val_acc: 0.0362\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.92975\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 56s 564ms/step - loss: 4.9063 - acc: 0.0391 - val_loss: 4.9190 - val_acc: 0.0397\n",
      "\n",
      "Epoch 00027: val_loss improved from 4.92975 to 4.91898, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 4.8702 - acc: 0.0450 - val_loss: 4.9434 - val_acc: 0.0350\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 4.91898\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 4.8754 - acc: 0.0394 - val_loss: 4.8996 - val_acc: 0.0438\n",
      "\n",
      "Epoch 00029: val_loss improved from 4.91898 to 4.89955, saving model to RandomSearchResults/weights.best.from_scratch.hdf5\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 4.8385 - acc: 0.0437 - val_loss: 5.0182 - val_acc: 0.0353\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4.89955\n",
      "4 poolSize/4h strides \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 4 from 1 for 'max_pooling2d_15/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'max_pooling2d_15/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0486b22841d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#Third Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#Fourth Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    203\u001b[0m                                         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                                         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                                         data_format=self.data_format)\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\u001b[0m in \u001b[0;36m_pooling_function\u001b[1;34m(self, inputs, pool_size, strides, padding, data_format)\u001b[0m\n\u001b[0;32m    266\u001b[0m         output = K.pool2d(inputs, pool_size, strides,\n\u001b[0;32m    267\u001b[0m                           \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                           pool_mode='max')\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[1;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[0;32m   3976\u001b[0m         x = tf.nn.max_pool(x, pool_size, strides,\n\u001b[0;32m   3977\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3978\u001b[1;33m                            data_format=tf_data_format)\n\u001b[0m\u001b[0;32m   3979\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'avg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3980\u001b[0m         x = tf.nn.avg_pool(x, pool_size, strides,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[1;34m(value, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[0;32m   2746\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2747\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2748\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[1;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[0;32m   5135\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   5136\u001b[0m         \u001b[1;34m\"MaxPool\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5137\u001b[1;33m                    data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m   5138\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5139\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'max_pooling2d_15/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,128]."
     ]
    }
   ],
   "source": [
    "\n",
    "filtersList = [32]\n",
    "kernelList = [5,6]\n",
    "stridesList = [2]\n",
    "poolList = [4]\n",
    "GlobalLoss = 35463548613541\n",
    "n = 10\n",
    "for strides in stridesList:\n",
    "    for pool_size in poolList:\n",
    "#         print('\\r{}/{}'. format(i, n))\n",
    "         #RandomFilter = random.sample(filtersList,1)\n",
    "         #RandomKernel = random.sample(kernelList,1)\n",
    "      #  RandomStrides = random.sample(stridesList,1)\n",
    "       # RandomPool = random.sample(poolList,1)\n",
    "         #filters = RandomFilter[0]\n",
    "         #kernel_size = RandomKernel[0]\n",
    "      #  strides = RandomStrides[0]\n",
    "      #  pool_size = RandomPool[0]\n",
    "        filters = 32\n",
    "        kernel_size = 6\n",
    "        print('starting with strides \\r{} poolSize/{}'. format(pool_size, strides))\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        #First Layer\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size,padding='same', input_shape=(128, 128, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #Second Layer\n",
    "        model.add(Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #Third Layer\n",
    "        model.add(Conv2D(filters=filters*4, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size,strides = strides))\n",
    "        model.add(Dropout(0.25))\n",
    "        #Fourth Layer\n",
    "        # model.add(Conv2D(filters=256, kernel_size=2,padding='same', activation='relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=2,strides = 4))\n",
    "        #model.add(Dropout(0.25))\n",
    "\n",
    "        #model.add(GlobalAveragePooling2D())\n",
    "        #Last Layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(196, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        epochs = 30\n",
    "        checkpointer = ModelCheckpoint(filepath='RandomSearchResults/weights.best.from_scratch.hdf5', \n",
    "                                   verbose=1, save_best_only=True)\n",
    "        Saving = model.fit_generator(train_data,\n",
    "              steps_per_epoch=100,  \n",
    "              epochs=epochs,\n",
    "              verbose=1, \n",
    "              validation_data=test_data,\n",
    "              validation_steps=100,\n",
    "              callbacks=[checkpointer])\n",
    "        LastMinValLoss = np.min(Saving.history['val_loss'])\n",
    "        if(LastMinValLoss < GlobalLoss):\n",
    "            GlobalLoss = LastMinValLoss\n",
    "            #save the best model so far\n",
    "            model.load_weights('RandomSearchResults/weights.best.from_scratch.hdf5')\n",
    "            model.save('RandomSearchResults/best_model.h5')\n",
    "            print('saving new best model, val_loss = {}'.format(LastMinValLoss))\n",
    "            #save best hyperparameters\n",
    "            BestFilter = filters\n",
    "            BestKernel = kernel_size\n",
    "            BestStrides = strides\n",
    "            BestPool = pool_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-51c8e42c654b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vals' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style({'xtick.bottom':False,\n",
    "               'ytick.left':False,\n",
    "               'axes.spines.bottom': False,\n",
    "               'axes.spines.left': False,\n",
    "               'axes.spines.right': False,\n",
    "               'axes.spines.top': False})\n",
    "\n",
    "ig,(ax,ax1) = plt.subplots(nrows=2,ncols=1,figsize=(20,20))\n",
    "sns.scatterplot(x='epochs',y='acc',data=vals,ax=ax,color='r')\n",
    "sns.lineplot(x='epochs',y='val_acc',data=vals,ax=ax,color='g')\n",
    "sns.scatterplot(x='epochs',y='loss',data=vals,ax=ax1,color='r')\n",
    "sns.lineplot(x='epochs',y='val_loss',data=vals,ax=ax1,color='g')\n",
    "ax.legend(labels=['Test Accuracy','Training Accuracy'])\n",
    "ax1.legend(labels=['Test Loss','Training Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
